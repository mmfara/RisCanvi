{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCAs5bD8n5oJXHp/fB2nhl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/RisCanvi/blob/main/RisCanvi_and_COMPAS_Statistical_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RISCANVI**"
      ],
      "metadata": {
        "id": "H7HIkRUFMVl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Standard Oversampling Strategy**"
      ],
      "metadata": {
        "id": "dx99bc3kMcdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "The Logistic Regression (LR) model exhibits the highest level of disparate impact with a mean value of **0.49**, suggesting that it significantly disadvantages certain demographic groups. K-Nearest Neighbors (KNN) has the most balanced impact at **0.67**, indicating that it makes more equitable decisions across different groups. Other models, including Random Forest (RF), Support Vector Machine (SVM), Decision Tree (DT), Naïve Bayes (NB), and Gradient Boosting (GraB), fall between these two extremes, with values ranging from **0.55 to 0.64**. This suggests that while none of the models achieve perfect fairness, some perform considerably worse than others in terms of how their predictions are distributed across groups.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "Logistic Regression (LR) has the most negative statistical parity difference (**-0.166**), meaning it disproportionately favors one group over another when making selections. KNN, with the smallest disparity (**-0.059**), demonstrates the least bias in this regard. Models like Random Forest (RF), SVM, Decision Tree (DT), Naïve Bayes (NB), and Gradient Boosting (GraB) show moderate disparities, with SPD values ranging between **-0.09 and -0.11**. This suggests that while KNN provides relatively equitable predictions, the other models still exhibit some level of disparity in selection rates.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "Logistic Regression (LR) again shows the highest level of unfairness, with an equal opportunity difference of **-0.271**, indicating that it significantly underpredicts favorable outcomes for the disadvantaged group. KNN has the best fairness score (**-0.097**), suggesting it maintains a more consistent true positive rate across different groups. Random Forest (RF), SVM, Decision Tree (DT), Naïve Bayes (NB), and Gradient Boosting (GraB) all show moderate bias, with values ranging from **-0.13 to -0.20**. This means that while KNN is the best in ensuring fair opportunity, other models struggle to provide equal access to favorable outcomes.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "Logistic Regression (LR) again demonstrates the worst fairness in this category, with a predictive equality difference of **-0.142**, meaning that it falsely predicts negative outcomes for the disadvantaged group at a significantly higher rate. KNN performs best (**-0.049**), indicating that its false positive rates are more evenly distributed. Other models, such as Random Forest (RF), SVM, Decision Tree (DT), Naïve Bayes (NB), and Gradient Boosting (GraB), have moderate disparities, ranging from **-0.07 to -0.09**. While these models are not as biased as LR, they still exhibit non-trivial differences in their false positive rates across demographic groups.\n",
        "\n",
        "## Accuracy\n",
        "Random Forest (RF) is the most accurate model, achieving a mean accuracy of **0.784**, followed closely by K-Nearest Neighbors (KNN) at **0.773** and Support Vector Machine (SVM) at **0.765**. Gradient Boosting (GraB) also performs well, with an accuracy of **0.762**. Decision Tree (DT) and Logistic Regression (LR) fall slightly behind at **0.745** and **0.737**, respectively. Naïve Bayes (NB) has the lowest accuracy at **0.727**. While RF offers the best overall performance in terms of predictive capability, fairness concerns need to be addressed, as it does not perform optimally across all fairness metrics. KNN provides a strong balance between accuracy and fairness, making it a favorable choice if both factors are important.\n",
        "\n"
      ],
      "metadata": {
        "id": "3T3Bzv10Mul6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Oversampling Based on Sensitive Attributes Strategy**"
      ],
      "metadata": {
        "id": "2af-vwmYNk-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Logistic Regression (LR) has the lowest disparate impact (**0.44**), suggesting that it disproportionately affects certain demographic groups more than other models. In contrast, K-Nearest Neighbors (KNN) has a DI of **1.16**, which is unusually high and suggests overcompensation, potentially favoring the disadvantaged group. Random Forest (RF) has a DI of **1.00**, indicating a nearly balanced prediction impact. Support Vector Machine (SVM), Decision Tree (DT), Naïve Bayes (NB), and Gradient Boosting (GraB) fall in between, with DI values ranging from **0.52 to 0.76**, indicating varying degrees of bias but still being below the ideal fairness threshold of **0.8**.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "Most models show a near-zero statistical parity difference, indicating relatively fair selection rates. Logistic Regression (LR) has an SPD of **-0.023**, meaning it slightly favors one group over another. KNN (**0.006**) and RF (**-0.002**) have the smallest differences, suggesting that these models provide the most balanced outcomes. SVM (**-0.001**) is almost perfectly balanced. However, Naïve Bayes (NB) stands out with an SPD of **-0.112**, indicating a significant bias in favor of one group over another. Gradient Boosting (GraB) and Decision Tree (DT) show moderate biases (**-0.027** and **-0.028**, respectively).\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "LR has an equal opportunity difference of **-0.086**, showing moderate bias in correctly predicting positive outcomes for different groups. KNN (**-0.074**), RF (**-0.067**), and DT (**-0.069**) show similar levels of moderate bias, with KNN being slightly better. SVM (**-0.015**) is the fairest, indicating nearly equal true positive rates across groups. Naïve Bayes (NB), however, performs the worst (**-0.222**), meaning it strongly underpredicts positive outcomes for the disadvantaged group. GraB (**-0.103**) also has considerable bias but is not as extreme as NB.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "Most models perform well in predictive equality, with very small differences in false positive rates across groups. LR (**-0.010**), RF (**0.011**), and SVM (**0.001**) show minimal bias, suggesting nearly equal treatment of false positives. KNN (**0.022**) slightly favors the disadvantaged group, while DT (**-0.019**) and GraB (**-0.011**) show small negative biases. NB again stands out as the most biased (**-0.087**), indicating a significant disparity in false positive rates between groups.\n",
        "\n",
        "## Accuracy\n",
        "RF (**0.834**) and LR (**0.834**) perform best in terms of accuracy, followed closely by SVM (**0.832**). KNN (**0.821**) and Gradient Boosting (**0.824**) also perform well. Decision Tree (**0.799**) is slightly lower in accuracy, while Naïve Bayes (**0.728**) has the lowest accuracy, suggesting it is the weakest performer overall. RF and LR seem to provide the best accuracy while maintaining a relatively fair distribution across fairness metrics.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** SVM performs the best across fairness metrics, showing the lowest disparities while maintaining high accuracy.\n",
        "- **Most Accurate Model:** RF and LR provide the best accuracy, but LR has higher fairness concerns.\n",
        "- **Most Biased Model:** Naïve Bayes shows significant disparities across multiple fairness metrics, making it the least fair model.\n",
        "- **Best Trade-Off:** RF offers the best balance between accuracy and fairness, though some minor disparities remain.\n",
        "\n"
      ],
      "metadata": {
        "id": "ctO92CLCNn7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Oversampling Based on Sensitive Attributes Strategy**"
      ],
      "metadata": {
        "id": "JGSekyqcN4nG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Logistic Regression (LR) and Random Forest (RF) exhibit extremely high disparate impact values (**2.94** and **3.60**, respectively), suggesting a strong tendency to favor the disadvantaged group, potentially leading to overcompensation. Gradient Boosting (GraB) also has a high DI (**2.10**), though not as extreme. On the other hand, Naïve Bayes (NB) has a DI of **0.89**, meaning it treats groups more equally but slightly underrepresents the disadvantaged group. Models like K-Nearest Neighbors (KNN), Decision Tree (DT), and Support Vector Machine (SVM) have DI values closer to **1.0–1.7**, indicating a more balanced distribution of outcomes, though SVM and KNN show slightly more favorable treatment towards one group.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "LR (**0.14**) and RF (**0.096**) have relatively high positive SPD values, meaning they are favoring one group significantly more in selection rates. Gradient Boosting (**0.089**) and SVM (**0.047**) also show bias in the same direction, though to a lesser extent. KNN (**0.04**) and DT (**0.042**) are closer to neutral, meaning they make more balanced selections. Naïve Bayes (NB), with an SPD of **-0.024**, is the only model slightly favoring the advantaged group, though the bias is mild. Overall, KNN and DT seem to have the least disparity in selection rates, while LR and RF may require fairness adjustments.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "LR (**0.22**) exhibits the highest disparity in true positive rates, meaning it is significantly favoring one group in terms of correctly predicting positive outcomes. RF (**0.093**), Gradient Boosting (**0.101**), and SVM (**0.075**) also show mild favoritism but not as extreme. KNN (**-0.027**) and DT (**0.015**) are the most balanced, while Naïve Bayes (**-0.094**) is the only model that negatively impacts the disadvantaged group, showing significant bias in the opposite direction. This suggests that while most models are slightly favoring one group, NB disproportionately disadvantages them, making it the most unfair in this category.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "LR (**0.13**) again shows the highest disparity, meaning its false positive rates differ significantly between groups, favoring one over the other. RF (**0.098**) and Gradient Boosting (**0.089**) also exhibit similar disparities. KNN (**0.055**), SVM (**0.044**), and DT (**0.048**) are relatively balanced in predictive equality. NB (**-0.008**) is the closest to zero, meaning it has the least false positive rate disparity, making it the fairest model in this category. However, given NB’s poor performance in equal opportunity, this might not necessarily translate into overall fairness.\n",
        "\n",
        "## Accuracy\n",
        "RF (**0.815**), SVM (**0.819**), and KNN (**0.812**) achieve the highest accuracy, making them the best-performing models in terms of predictive capability. LR (**0.798**) and Gradient Boosting (**0.792**) follow closely behind. Decision Tree (**0.767**) and Naïve Bayes (**0.753**) have the lowest accuracy, meaning they are the weakest models overall. SVM and RF stand out as strong performers, offering a balance between accuracy and fairness, though RF may require fairness adjustments to avoid significant disparate impact.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** KNN is the best-balanced model, showing minimal bias across most fairness metrics while maintaining high accuracy.\n",
        "- **Most Accurate Model:** SVM and RF are the most accurate, but RF has serious fairness concerns.\n",
        "- **Most Biased Model:** LR and RF exhibit the highest bias, particularly in disparate impact and selection disparities.\n",
        "- **Best Trade-Off:** KNN and SVM offer a good balance between accuracy and fairness, making them the most preferable choices.\n",
        "\n"
      ],
      "metadata": {
        "id": "UNyp_XXsN-o_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Equalized Discrimination Group Instrances Strategy**"
      ],
      "metadata": {
        "id": "OpcER_T5OU3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "K-Nearest Neighbors (KNN) exhibits the lowest disparate impact (**0.48**), suggesting that it disproportionately disadvantages a specific group more than other models. Logistic Regression (LR), Support Vector Machine (SVM), and Gradient Boosting (GraB) have DI values closer to **1.0**, indicating more equitable treatment across groups. Random Forest (RF), Decision Tree (DT), and Naïve Bayes (NB) fall between **0.68 and 0.75**, meaning they exhibit some level of disparity but are not as extreme as KNN. This suggests that KNN is the most biased in this metric, while LR and SVM are relatively more balanced.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "KNN has the most negative SPD (**-0.141**), meaning it strongly favors one group over another in selection rates. Naïve Bayes (NB) (**-0.094**) and Decision Tree (DT) (**-0.059**) also show moderate bias, while Random Forest (RF) (**-0.055**) and Gradient Boosting (GraB) (**-0.042**) are slightly better but still show some disparities. LR (**-0.029**) and SVM (**-0.036**) are the closest to zero, meaning they provide the most balanced selection rates across different groups.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "KNN (**-0.142**) again has the highest disparity, meaning it underpredicts favorable outcomes for one group at a significant rate. RF (**-0.085**), NB (**-0.093**), and DT (**-0.077**) also exhibit notable bias but to a lesser extent. Gradient Boosting (**-0.066**) and SVM (**-0.007**) show more balanced predictions, with SVM performing the best in this metric. LR (**0.007**) is also near zero, meaning it does not significantly favor one group over another in terms of true positive rates.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "KNN (**-0.124**) has the worst performance, meaning its false positive rates are significantly skewed against a particular group. NB (**-0.073**) and DT (**-0.042**) also show moderate disparities. RF (**-0.032**), SVM (**-0.026**), and Gradient Boosting (**-0.022**) show lower levels of bias. LR (**-0.014**) is the closest to zero, meaning it treats false positive rates more equally between groups, making it the most fair model in this category.\n",
        "\n",
        "## Accuracy\n",
        "Random Forest (**0.795**) is the most accurate model, followed by SVM (**0.773**) and Gradient Boosting (**0.773**). Logistic Regression (**0.748**), Decision Tree (**0.747**), and KNN (**0.743**) show slightly lower accuracy, with Naïve Bayes (**0.736**) performing the worst. RF stands out as the best-performing model in terms of predictive power.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** SVM performs the best across fairness metrics, showing the lowest disparities while maintaining high accuracy.\n",
        "- **Most Accurate Model:** RF provides the highest accuracy but has some fairness concerns.\n",
        "- **Most Biased Model:** KNN exhibits the highest bias across multiple fairness metrics, making it the least fair.\n",
        "- **Best Trade-Off:** SVM and RF offer a balance between accuracy and fairness, making them the most preferable choices.\n",
        "\n"
      ],
      "metadata": {
        "id": "TE3Wk-leOW1D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9E_unNnOZAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**COMPAS**"
      ],
      "metadata": {
        "id": "cf4or0R5OZwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Standard Oversampling Strategy**"
      ],
      "metadata": {
        "id": "l99N_7LdOmud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Naïve Bayes (NB) has the highest disparate impact (**2.97**), indicating that it strongly favors the disadvantaged group, potentially to the point of overcompensation. Logistic Regression (LR), Decision Tree (DT), and Random Forest (RF) also have high DI values (**ranging from 2.03 to 2.06**), meaning they provide significantly different treatment between groups. Support Vector Machine (SVM) and Gradient Boosting (GraB) have slightly lower values (**1.80–1.83**), while K-Nearest Neighbors (KNN) has the most balanced DI (**1.64**), though still above the ideal range. The results suggest that most models exhibit a strong imbalance in prediction outcomes, with Naïve Bayes being the most extreme case.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "Random Forest (RF) and Logistic Regression (LR) have the highest SPD values (**0.265 and 0.254**), meaning they favor one group significantly more in selection rates. Decision Tree (DT), SVM, and Gradient Boosting (GraB) also show notable disparities (**0.23–0.24**), while KNN (**0.178**) and Naïve Bayes (**0.191**) have the smallest selection rate differences. However, none of the models achieve perfect fairness, as all have positive SPD values, indicating a bias towards selecting one group more frequently than the other.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "LR (**0.257**) and RF (**0.261**) again have the highest EOD values, suggesting that they significantly favor one group in terms of correctly predicting positive outcomes. Naïve Bayes (**0.255**) also shows a high level of bias, while SVM, Decision Tree, and Gradient Boosting have slightly lower disparities (**0.21–0.23**). KNN (**0.170**) is the closest to being balanced, but it still shows some bias. This suggests that models like RF and LR are consistently favoring one group in both selection and prediction accuracy.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "RF (**0.208**) has the highest PED, meaning it has the greatest disparity in false positive rates between groups. LR (**0.188**) and SVM (**0.186**) also show high levels of bias. KNN (**0.130**) and Naïve Bayes (**0.096**) exhibit the least bias, making them the most fair in this category. While NB performs best in terms of predictive equality, its disparate impact score suggests that it still strongly favors one group in overall predictions.\n",
        "\n",
        "## Accuracy\n",
        "Logistic Regression (**0.672**), Gradient Boosting (**0.669**), and SVM (**0.667**) have the highest accuracy, making them the best-performing models in terms of predictive power. Random Forest (**0.660**) follows closely, while KNN (**0.647**) and Decision Tree (**0.647**) show slightly lower accuracy. Naïve Bayes (**0.639**) performs the worst in terms of accuracy, suggesting it is the weakest model overall despite its better fairness in some areas.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** KNN has the lowest bias in most fairness metrics while maintaining reasonable accuracy.\n",
        "- **Most Accurate Model:** Logistic Regression (LR) provides the highest accuracy but has fairness concerns.\n",
        "- **Most Biased Model:** Random Forest (RF) and LR exhibit the most bias, particularly in disparate impact and selection disparities.\n",
        "- **Best Trade-Off:** Gradient Boosting (GraB) and SVM offer a good balance between accuracy and fairness, making them preferable choices if both factors are considered important.\n",
        "\n"
      ],
      "metadata": {
        "id": "51TzMSQIO-lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Oversampling Based on Sensitive Attributes Strategy**"
      ],
      "metadata": {
        "id": "SmTqLvjyPSMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Naïve Bayes (NB) has the highest disparate impact (**3.31**), indicating a strong overcompensation favoring the disadvantaged group. Logistic Regression (LR), Support Vector Machine (SVM), and Random Forest (RF) also have high DI values (**above 2.0**), suggesting significant disparity in how different demographic groups are treated. K-Nearest Neighbors (KNN) (**1.63**) has the most balanced DI among all models, though it still shows some bias. Decision Tree (DT) and Gradient Boosting (GraB) fall between **1.89 and 1.89**, indicating moderate disparities. Overall, most models exhibit strong disparities, with NB being the most extreme case.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "Logistic Regression (**0.245**) and Random Forest (**0.251**) have the highest SPD values, meaning they strongly favor one group over another in selection rates. Support Vector Machine (SVM) (**0.238**) and Gradient Boosting (**0.214**) also show bias in the same direction, though slightly less extreme. KNN (**0.177**) has the least disparity, meaning it provides the most balanced selection rates. Decision Tree (**0.201**) and Naïve Bayes (**0.202**) show moderate selection bias.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "Naïve Bayes (**0.273**) has the highest EOD, meaning it significantly favors one group in terms of correctly predicting positive outcomes. Logistic Regression (**0.252**) and Random Forest (**0.248**) also show strong disparities, with SVM (**0.240**) and Decision Tree (**0.200**) following closely behind. KNN (**0.168**) has the lowest EOD, meaning it is the most balanced model in terms of true positive rates. This suggests that models like LR, RF, and NB may need fairness adjustments to improve equal opportunity.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "Random Forest (**0.196**) has the highest PED, meaning it has the greatest disparity in false positive rates between groups. Logistic Regression (**0.176**) and SVM (**0.175**) also show notable levels of bias. KNN (**0.133**) has the least disparity, making it the most fair in this category. Naïve Bayes (**0.102**) performs the best in predictive equality, but its high disparate impact suggests that it still strongly favors one group in overall predictions.\n",
        "\n",
        "## Accuracy\n",
        "Logistic Regression (**0.674**) and SVM (**0.669**) have the highest accuracy, making them the best-performing models in terms of predictive power. Random Forest (**0.656**) and Gradient Boosting (**0.656**) follow closely. KNN (**0.639**), Decision Tree (**0.643**), and Naïve Bayes (**0.637**) have the lowest accuracy, suggesting that they are the weakest models overall.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** KNN has the lowest bias across most fairness metrics while maintaining reasonable accuracy.\n",
        "- **Most Accurate Model:** Logistic Regression (LR) and SVM provide the highest accuracy but have fairness concerns.\n",
        "- **Most Biased Model:** Naïve Bayes (NB) exhibits the most bias, particularly in disparate impact and equal opportunity difference.\n",
        "- **Best Trade-Off:** Gradient Boosting (GraB) and SVM offer a good balance between accuracy and fairness, making them preferable choices if both factors are considered important.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVB2u3_GPThX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Oversampling Based on Sensitive Attributes Strategy**"
      ],
      "metadata": {
        "id": "6J8ACXaGPZXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Naïve Bayes (NB) has the highest disparate impact (**3.01**), indicating that it strongly favors one demographic group, potentially overcompensating. Decision Tree (DT), Gradient Boosting (GraB), Random Forest (RF), and Support Vector Machine (SVM) also exhibit high DI values (**1.91–1.98**), meaning they significantly favor one group over another. K-Nearest Neighbors (KNN) has the lowest DI (**1.50**), suggesting it is the most balanced model in terms of proportionate treatment across groups, though still not ideal. Overall, most models exhibit considerable disparities, with NB being the most extreme case.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "Random Forest (RF) (**0.229**) and Gradient Boosting (GraB) (**0.227**) have the highest SPD values, meaning they strongly favor one group over another in selection rates. Support Vector Machine (SVM) (**0.212**) and Decision Tree (DT) (**0.212**) also show bias in the same direction. KNN (**0.148**) has the lowest disparity, making it the most balanced in selection rates. Naïve Bayes (**0.175**) and Logistic Regression (LR) (**0.203**) fall in between, suggesting moderate bias in selection outcomes.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "Naïve Bayes (**0.240**) has the highest EOD, meaning it significantly favors one group in terms of correctly predicting positive outcomes. Random Forest (**0.216**), Gradient Boosting (**0.211**), and SVM (**0.221**) also show notable disparities. KNN (**0.124**) has the lowest EOD, meaning it is the most balanced model in terms of true positive rates. This suggests that models like RF, SVM, and NB need fairness adjustments to ensure more equitable outcomes.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "Random Forest (**0.180**) and Gradient Boosting (**0.179**) have the highest PED values, meaning they exhibit the largest disparities in false positive rates. Logistic Regression (**0.135**) and KNN (**0.115**) perform slightly better but still show notable differences. Naïve Bayes (**0.085**) is the most fair in this category, suggesting that it has the most balanced false positive rates between groups.\n",
        "\n",
        "## Accuracy\n",
        "Logistic Regression (**0.675**) has the highest accuracy, making it the best-performing model in terms of predictive power. Gradient Boosting (**0.665**), Support Vector Machine (**0.660**), and Random Forest (**0.659**) follow closely behind. KNN (**0.637**), Decision Tree (**0.649**), and Naïve Bayes (**0.626**) have the lowest accuracy, suggesting they are the weakest models overall.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** KNN has the lowest bias across most fairness metrics while maintaining reasonable accuracy.\n",
        "- **Most Accurate Model:** Logistic Regression (LR) provides the highest accuracy but has some fairness concerns.\n",
        "- **Most Biased Model:** Naïve Bayes (NB) exhibits the most bias, particularly in disparate impact and equal opportunity difference.\n",
        "- **Best Trade-Off:** Gradient Boosting (GraB) and SVM offer a good balance between accuracy and fairness, making them preferable choices if both factors are important.\n",
        "\n"
      ],
      "metadata": {
        "id": "1j9ZzhyuPjrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Equalized Discrimination Group Instrances Strategy**"
      ],
      "metadata": {
        "id": "1wuBp-pxPxx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Disparate Impact (DI)\n",
        "Naïve Bayes (NB) has the highest disparate impact (**2.51**), indicating a strong tendency to favor the disadvantaged group, potentially leading to overcompensation. K-Nearest Neighbors (KNN) (**1.59**) and Support Vector Machine (SVM) (**1.50**) also show notable bias, though less extreme. Logistic Regression (LR) (**1.25**) and Gradient Boosting (GraB) (**1.24**) have the lowest DI values, suggesting a more balanced distribution of outcomes, though they still indicate some level of disparity. Random Forest (RF) (**1.40**) and Decision Tree (DT) (**1.38**) fall in between. These results suggest that while some models provide more balanced predictions, others (particularly NB) have a significant bias.\n",
        "\n",
        "## Statistical Parity Difference (SPD)\n",
        "NB (**0.159**) and KNN (**0.164**) show the highest SPD, meaning they favor one group in selection rates. SVM (**0.154**) and RF (**0.123**) also exhibit moderate disparities. Logistic Regression (**0.075**) and Gradient Boosting (**0.075**) have the lowest SPD, suggesting they provide the most balanced selection rates. Decision Tree (**0.102**) shows moderate bias. Overall, LR and GraB are the most balanced in terms of selection rates, while KNN and NB are the most biased.\n",
        "\n",
        "## Equal Opportunity Difference (EOD)\n",
        "Naïve Bayes (**0.205**) again has the highest EOD, meaning it significantly favors one group in correctly predicting positive outcomes. KNN (**0.146**) and SVM (**0.142**) also show notable disparities. LR (**0.073**) and GraB (**0.067**) have the lowest EOD, indicating that they provide the most equal opportunity between groups. Decision Tree (**0.100**) and RF (**0.113**) show moderate bias. These results suggest that LR and GraB offer more balanced true positive rates, while NB and KNN exhibit higher disparities.\n",
        "\n",
        "## Predictive Equality Difference (PED)\n",
        "KNN (**0.129**) and SVM (**0.102**) have the highest PED, meaning their false positive rates differ the most between groups. NB (**0.080**) and RF (**0.072**) show moderate disparities. LR (**0.011**) and GraB (**0.020**) have the lowest PED, meaning they treat false positives more equally across demographic groups. Decision Tree (**0.048**) also shows relatively low bias. This suggests that LR and GraB provide the most fair false positive rates, while KNN and SVM exhibit the most disparity.\n",
        "\n",
        "## Accuracy\n",
        "Logistic Regression (**0.666**) and Support Vector Machine (**0.661**) have the highest accuracy, making them the best models in terms of predictive performance. Gradient Boosting (**0.654**) and Random Forest (**0.650**) also perform well. Decision Tree (**0.643**) and KNN (**0.635**) show slightly lower accuracy, while Naïve Bayes (**0.631**) has the lowest accuracy. This suggests that while NB exhibits the highest fairness disparities, it also performs the worst in predictive accuracy.\n",
        "\n",
        "## Overall Conclusion\n",
        "- **Most Fair Model:** Logistic Regression (LR) and Gradient Boosting (GraB) have the lowest fairness disparities across all metrics while maintaining reasonable accuracy.\n",
        "- **Most Accurate Model:** Logistic Regression (LR) and Support Vector Machine (SVM) provide the highest accuracy but have some fairness concerns.\n",
        "- **Most Biased Model:** Naïve Bayes (NB) exhibits the most bias, particularly in disparate impact and equal opportunity difference.\n",
        "- **Best Trade-Off:** Gradient Boosting (GraB) and Logistic Regression (LR) offer the best balance between accuracy and fairness, making them the most preferable choices."
      ],
      "metadata": {
        "id": "Vw0RYIE_P00c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qr4_B-teQoYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comparison of the Best Algorithm Across Both Datasets (RisCanvi & COMPAS)**  \n",
        "\n"
      ],
      "metadata": {
        "id": "kZT1CnckQoBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the best algorithm across both datasets, we compare the performance in **fairness metrics (Disparate Impact, Statistical Parity Difference, Equal Opportunity Difference, Predictive Equality Difference)** and accuracy.\n",
        "\n",
        "### 1. Disparate Impact (DI)\n",
        "- **Best fairness (closer to 1.0 is better)**  \n",
        "  - RisCanvi: Logistic Regression (0.88), SVM (0.84), and Gradient Boosting (0.79) are closest to 1.0.  \n",
        "  - COMPAS: Logistic Regression (1.25) and Gradient Boosting (1.24) are closest to 1.0.  \n",
        "  - Naïve Bayes (NB) is the worst in both datasets, favoring one group (0.69 in RisCanvi, 2.51 in COMPAS).  \n",
        "  - **Overall Best:** Logistic Regression (LR) and Gradient Boosting (GraB) are the most balanced across both datasets.  \n",
        "\n",
        "### 2. Statistical Parity Difference (SPD)\n",
        "- **Best fairness (closer to 0 is better)**  \n",
        "  - RisCanvi: Logistic Regression (-0.029) and Gradient Boosting (-0.042) are closest to 0.  \n",
        "  - COMPAS: Logistic Regression (0.075) and Gradient Boosting (0.075) are also closest to 0.  \n",
        "  - KNN and NB show the most disparity in both datasets.  \n",
        "  - **Overall Best:** Logistic Regression (LR) and Gradient Boosting (GraB).  \n",
        "\n",
        "### 3. Equal Opportunity Difference (EOD)\n",
        "- **Best fairness (closer to 0 is better)**  \n",
        "  - RisCanvi: Logistic Regression (0.007) and SVM (-0.007) perform best.  \n",
        "  - COMPAS: Logistic Regression (0.073) and Gradient Boosting (0.067) perform best.  \n",
        "  - Naïve Bayes (NB) is the most biased in both datasets (0.205 in COMPAS, -0.093 in RisCanvi).  \n",
        "  - **Overall Best:** Logistic Regression (LR) and Gradient Boosting (GraB).  \n",
        "\n",
        "### 4. Predictive Equality Difference (PED)\n",
        "- **Best fairness (closer to 0 is better)**  \n",
        "  - RisCanvi: Logistic Regression (-0.014) and Gradient Boosting (-0.022) perform best.  \n",
        "  - COMPAS: Logistic Regression (0.011) and Gradient Boosting (0.020) perform best.  \n",
        "  - KNN shows the most bias in both datasets.  \n",
        "  - **Overall Best:** Logistic Regression (LR) and Gradient Boosting (GraB).  \n",
        "\n",
        "### 5. Accuracy\n",
        "- **Best performance (higher is better)**  \n",
        "  - RisCanvi: Random Forest (0.795), Gradient Boosting (0.773), and SVM (0.773) perform best.  \n",
        "  - COMPAS: Logistic Regression (0.666), SVM (0.661), and Gradient Boosting (0.654) perform best.  \n",
        "  - Naïve Bayes (NB) has the worst accuracy in both datasets.  \n",
        "  - **Overall Best:** SVM and Gradient Boosting (GraB) are the strongest across both datasets.  \n",
        "\n",
        "### Final Verdict: Best Algorithm Across Both Datasets\n",
        "- **Best Overall Model:** Gradient Boosting (GraB)\n",
        "  - Balanced fairness across all metrics.\n",
        "  - High accuracy (0.773 in RisCanvi, 0.654 in COMPAS).\n",
        "  - Among the lowest disparities in all fairness measures.\n",
        "  \n",
        "- **Runner-Up:** Logistic Regression (LR)\n",
        "  - Very balanced fairness across both datasets.\n",
        "  - Good accuracy (0.748 in RisCanvi, 0.666 in COMPAS).\n",
        "  - Performs slightly worse than GraB in some fairness metrics but remains strong.\n",
        "\n",
        "### Conclusion\n",
        "**Gradient Boosting (GraB)** is the best-performing model across both datasets, providing the best balance of fairness and accuracy.  \n",
        "**Logistic Regression (LR)** is a close second, performing well in fairness but slightly lower in accuracy.  \n",
        "If fairness is the highest priority, Gradient Boosting (GraB) is the best choice across both datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "aDq2wThZQaLk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNzGgKUZO_FM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}